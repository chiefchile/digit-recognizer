{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 1\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from datetime import datetime\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "onehot.fit(df[[\"label\"]])\n",
    "cat_list = np.concatenate(onehot.categories_)\n",
    "df[cat_list] = pd.DataFrame(onehot.transform(df[[\"label\"]]).toarray(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 784) (33600, 10)\n",
      "(8400, 784) (8400, 10)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=cat_list)\n",
    "X = X.drop(columns=[\"label\"])\n",
    "y = df[cat_list]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogUniform:\n",
    "    def __init__(self, low, high):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "    \n",
    "    def rvs(self, *args, **kwds):\n",
    "        return 10 ** np.random.uniform(np.log10(self.low), np.log10(self.high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28 \n",
      "WARNING:tensorflow:From d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\alex\\code\\machine-learning\\env36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "26880/26880 [==============================] - 3s 100us/step - loss: 12.6329 - acc: 0.4044\n",
      "Epoch 2/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 10.5947 - acc: 0.4766\n",
      "Epoch 3/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 9.9486 - acc: 0.4843\n",
      "Epoch 4/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 9.5166 - acc: 0.4867\n",
      "Epoch 5/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 9.1755 - acc: 0.4912\n",
      "Epoch 6/50\n",
      "26880/26880 [==============================] - 3s 93us/step - loss: 8.9535 - acc: 0.4916\n",
      "Epoch 7/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 8.7674 - acc: 0.4939\n",
      "Epoch 8/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 8.6228 - acc: 0.4940\n",
      "Epoch 9/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 8.4989 - acc: 0.4953\n",
      "Epoch 10/50\n",
      "26880/26880 [==============================] - 2s 93us/step - loss: 2.3666 - acc: 0.7779: 0s - loss: 3.0476\n",
      "Epoch 11/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.5659 - acc: 0.9193\n",
      "Epoch 12/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.4215 - acc: 0.9350\n",
      "Epoch 13/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.3389 - acc: 0.9425\n",
      "Epoch 14/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2967 - acc: 0.9470\n",
      "Epoch 15/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2676 - acc: 0.9519\n",
      "Epoch 16/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2523 - acc: 0.9536\n",
      "Epoch 17/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2421 - acc: 0.9566\n",
      "Epoch 18/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2327 - acc: 0.9584\n",
      "Epoch 19/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2274 - acc: 0.9594\n",
      "Epoch 20/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2275 - acc: 0.9582\n",
      "Epoch 21/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2208 - acc: 0.9597\n",
      "Epoch 22/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2182 - acc: 0.9601\n",
      "Epoch 23/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2150 - acc: 0.9612\n",
      "Epoch 24/50\n",
      "26880/26880 [==============================] - 2s 92us/step - loss: 0.2162 - acc: 0.9612\n",
      "Epoch 25/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2123 - acc: 0.9625\n",
      "Epoch 26/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 0.2135 - acc: 0.9623\n",
      "Epoch 27/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2094 - acc: 0.9616\n",
      "Epoch 28/50\n",
      "26880/26880 [==============================] - 2s 92us/step - loss: 0.2115 - acc: 0.9625\n",
      "Epoch 29/50\n",
      "26880/26880 [==============================] - 3s 93us/step - loss: 0.2082 - acc: 0.9638\n",
      "Epoch 30/50\n",
      "26880/26880 [==============================] - 2s 93us/step - loss: 0.2100 - acc: 0.9618\n",
      "Epoch 31/50\n",
      "26880/26880 [==============================] - 2s 92us/step - loss: 0.2106 - acc: 0.9632\n",
      "Epoch 32/50\n",
      "26880/26880 [==============================] - 3s 93us/step - loss: 0.2092 - acc: 0.9628\n",
      "Epoch 33/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2090 - acc: 0.9639\n",
      "Epoch 34/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2080 - acc: 0.9641\n",
      "Epoch 35/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2077 - acc: 0.9642\n",
      "Epoch 36/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 0.2065 - acc: 0.9645\n",
      "Epoch 37/50\n",
      "26880/26880 [==============================] - 3s 95us/step - loss: 0.2066 - acc: 0.9639\n",
      "Epoch 38/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2068 - acc: 0.9655\n",
      "Epoch 39/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2056 - acc: 0.9652\n",
      "Epoch 40/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2081 - acc: 0.9638\n",
      "Epoch 41/50\n",
      "26880/26880 [==============================] - 2s 93us/step - loss: 0.2057 - acc: 0.9649\n",
      "Epoch 42/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2056 - acc: 0.9655\n",
      "Epoch 43/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2034 - acc: 0.9659\n",
      "Epoch 44/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2024 - acc: 0.9657\n",
      "Epoch 45/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2048 - acc: 0.9649\n",
      "Epoch 46/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 0.2053 - acc: 0.9653\n",
      "Epoch 47/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2026 - acc: 0.9656\n",
      "Epoch 48/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2015 - acc: 0.9656\n",
      "Epoch 49/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2026 - acc: 0.9667\n",
      "Epoch 50/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2037 - acc: 0.9659\n",
      "6720/6720 [==============================] - 0s 38us/step\n",
      "[CV]  batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28, score=0.952, total= 2.0min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28 \n",
      "Epoch 1/50\n",
      "26880/26880 [==============================] - 3s 97us/step - loss: 11.3347 - acc: 0.5062\n",
      "Epoch 2/50\n",
      "26880/26880 [==============================] - 2s 92us/step - loss: 6.6183 - acc: 0.7506\n",
      "Epoch 3/50\n",
      "26880/26880 [==============================] - 2s 92us/step - loss: 4.4829 - acc: 0.8571\n",
      "Epoch 4/50\n",
      "26880/26880 [==============================] - 2s 81us/step - loss: 3.6793 - acc: 0.8892\n",
      "Epoch 5/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 3.1957 - acc: 0.9019\n",
      "Epoch 6/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 2.8021 - acc: 0.9136\n",
      "Epoch 7/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 2.4991 - acc: 0.9194\n",
      "Epoch 8/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 2.2357 - acc: 0.9264\n",
      "Epoch 9/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 2.0175 - acc: 0.9296\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26880/26880 [==============================] - 2s 79us/step - loss: 1.8102 - acc: 0.9330\n",
      "Epoch 11/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 1.6039 - acc: 0.9405\n",
      "Epoch 12/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 1.4287 - acc: 0.9418\n",
      "Epoch 13/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 1.2639 - acc: 0.9449\n",
      "Epoch 14/50\n",
      "26880/26880 [==============================] - 2s 82us/step - loss: 1.1209 - acc: 0.9445\n",
      "Epoch 15/50\n",
      "26880/26880 [==============================] - 2s 81us/step - loss: 0.9817 - acc: 0.9474: 0s - loss: 1.009\n",
      "Epoch 16/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 0.8448 - acc: 0.9505\n",
      "Epoch 17/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 0.7338 - acc: 0.9501\n",
      "Epoch 18/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 0.6306 - acc: 0.9501: 1s - los\n",
      "Epoch 19/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 0.5296 - acc: 0.9529\n",
      "Epoch 20/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 0.4554 - acc: 0.9556\n",
      "Epoch 21/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 0.3898 - acc: 0.9568\n",
      "Epoch 22/50\n",
      "26880/26880 [==============================] - 2s 81us/step - loss: 0.3397 - acc: 0.9598\n",
      "Epoch 23/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 0.3141 - acc: 0.9583\n",
      "Epoch 24/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2780 - acc: 0.9615\n",
      "Epoch 25/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 0.2582 - acc: 0.9611\n",
      "Epoch 26/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 0.2483 - acc: 0.9625\n",
      "Epoch 27/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 0.2366 - acc: 0.9618\n",
      "Epoch 28/50\n",
      "26880/26880 [==============================] - 2s 79us/step - loss: 0.2313 - acc: 0.9629\n",
      "Epoch 29/50\n",
      "26880/26880 [==============================] - 2s 82us/step - loss: 0.2250 - acc: 0.9633\n",
      "Epoch 30/50\n",
      "26880/26880 [==============================] - 2s 80us/step - loss: 0.2193 - acc: 0.9626\n",
      "Epoch 31/50\n",
      "26880/26880 [==============================] - 2s 82us/step - loss: 0.2184 - acc: 0.9632\n",
      "Epoch 32/50\n",
      "26880/26880 [==============================] - 2s 85us/step - loss: 0.2155 - acc: 0.9636\n",
      "Epoch 33/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2142 - acc: 0.9627\n",
      "Epoch 34/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2161 - acc: 0.9624\n",
      "Epoch 35/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2110 - acc: 0.9638\n",
      "Epoch 36/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2098 - acc: 0.9644\n",
      "Epoch 37/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2098 - acc: 0.9635\n",
      "Epoch 38/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2080 - acc: 0.9660\n",
      "Epoch 39/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2080 - acc: 0.9648\n",
      "Epoch 40/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2080 - acc: 0.9645\n",
      "Epoch 41/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2063 - acc: 0.9647\n",
      "Epoch 42/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2059 - acc: 0.9657\n",
      "Epoch 43/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2078 - acc: 0.9631\n",
      "Epoch 44/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2064 - acc: 0.9646\n",
      "Epoch 45/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2075 - acc: 0.9647\n",
      "Epoch 46/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2062 - acc: 0.9645\n",
      "Epoch 47/50\n",
      "26880/26880 [==============================] - 2s 85us/step - loss: 0.2036 - acc: 0.9665\n",
      "Epoch 48/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2040 - acc: 0.9650\n",
      "Epoch 49/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2049 - acc: 0.9645\n",
      "Epoch 50/50\n",
      "26880/26880 [==============================] - 2s 92us/step - loss: 0.2033 - acc: 0.9658\n",
      "6720/6720 [==============================] - 0s 37us/step\n",
      "[CV]  batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28, score=0.956, total= 1.9min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.9min remaining:    0.0s\n",
      "[CV] batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28 \n",
      "Epoch 1/50\n",
      "26880/26880 [==============================] - 3s 98us/step - loss: 13.0733 - acc: 0.3828\n",
      "Epoch 2/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 10.7585 - acc: 0.4699\n",
      "Epoch 3/50\n",
      "26880/26880 [==============================] - 3s 94us/step - loss: 8.9168 - acc: 0.5514\n",
      "Epoch 4/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 8.3400 - acc: 0.5672\n",
      "Epoch 5/50\n",
      "26880/26880 [==============================] - 2s 93us/step - loss: 8.0123 - acc: 0.5728\n",
      "Epoch 6/50\n",
      "26880/26880 [==============================] - 3s 93us/step - loss: 4.2301 - acc: 0.7633\n",
      "Epoch 7/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 3.2485 - acc: 0.8299\n",
      "Epoch 8/50\n",
      "26880/26880 [==============================] - 2s 85us/step - loss: 3.0173 - acc: 0.8403\n",
      "Epoch 9/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 1.5474 - acc: 0.9132\n",
      "Epoch 10/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 1.1940 - acc: 0.9338\n",
      "Epoch 11/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 1.0100 - acc: 0.9404\n",
      "Epoch 12/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.8600 - acc: 0.9430\n",
      "Epoch 13/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.7359 - acc: 0.9474\n",
      "Epoch 14/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.6329 - acc: 0.9502\n",
      "Epoch 15/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 0.5324 - acc: 0.9527\n",
      "Epoch 16/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.4658 - acc: 0.9528\n",
      "Epoch 17/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.3936 - acc: 0.9562\n",
      "Epoch 18/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.3391 - acc: 0.9598\n",
      "Epoch 19/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.3095 - acc: 0.9583\n",
      "Epoch 20/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2792 - acc: 0.9599\n",
      "Epoch 21/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2590 - acc: 0.9606\n",
      "Epoch 22/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2428 - acc: 0.9612\n",
      "Epoch 23/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2368 - acc: 0.9608\n",
      "Epoch 24/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2264 - acc: 0.9624\n",
      "Epoch 25/50\n",
      "26880/26880 [==============================] - 2s 92us/step - loss: 0.2222 - acc: 0.9628\n",
      "Epoch 26/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2191 - acc: 0.9614\n",
      "Epoch 27/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2113 - acc: 0.9646\n",
      "Epoch 28/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2103 - acc: 0.9642\n",
      "Epoch 29/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2097 - acc: 0.9641\n",
      "Epoch 30/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2080 - acc: 0.9653\n",
      "Epoch 31/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2087 - acc: 0.9641\n",
      "Epoch 32/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 0.2030 - acc: 0.9657\n",
      "Epoch 33/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2035 - acc: 0.9664\n",
      "Epoch 34/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2042 - acc: 0.9650\n",
      "Epoch 35/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2010 - acc: 0.9651\n",
      "Epoch 36/50\n",
      "26880/26880 [==============================] - 2s 90us/step - loss: 0.1996 - acc: 0.9665\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26880/26880 [==============================] - 2s 85us/step - loss: 0.2008 - acc: 0.9670\n",
      "Epoch 38/50\n",
      "26880/26880 [==============================] - 2s 85us/step - loss: 0.1976 - acc: 0.9662\n",
      "Epoch 39/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1995 - acc: 0.9656\n",
      "Epoch 40/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1970 - acc: 0.9682\n",
      "Epoch 41/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1979 - acc: 0.9664\n",
      "Epoch 42/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1973 - acc: 0.9672\n",
      "Epoch 43/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1955 - acc: 0.9673\n",
      "Epoch 44/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1971 - acc: 0.9663\n",
      "Epoch 45/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1978 - acc: 0.9671\n",
      "Epoch 46/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1974 - acc: 0.9672\n",
      "Epoch 47/50\n",
      "26880/26880 [==============================] - 2s 85us/step - loss: 0.1939 - acc: 0.9676\n",
      "Epoch 48/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1959 - acc: 0.9683\n",
      "Epoch 49/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1971 - acc: 0.9676\n",
      "Epoch 50/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.1936 - acc: 0.9683\n",
      "6720/6720 [==============================] - 0s 37us/step\n",
      "[CV]  batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28, score=0.956, total= 2.0min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.9min remaining:    0.0s\n",
      "[CV] batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28 \n",
      "Epoch 1/50\n",
      "26880/26880 [==============================] - 3s 94us/step - loss: 11.9696 - acc: 0.4664\n",
      "Epoch 2/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 8.2677 - acc: 0.6529\n",
      "Epoch 3/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 7.0248 - acc: 0.7044\n",
      "Epoch 4/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 5.1008 - acc: 0.8070\n",
      "Epoch 5/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 4.7014 - acc: 0.8175\n",
      "Epoch 6/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 4.3459 - acc: 0.8285: 0s - loss: 4.386\n",
      "Epoch 7/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 4.0767 - acc: 0.8339\n",
      "Epoch 8/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 3.8479 - acc: 0.8385\n",
      "Epoch 9/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 3.6428 - acc: 0.8433\n",
      "Epoch 10/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 2.9145 - acc: 0.8744\n",
      "Epoch 11/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 1.9516 - acc: 0.9253\n",
      "Epoch 12/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 1.7226 - acc: 0.9335\n",
      "Epoch 13/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 1.5386 - acc: 0.9382\n",
      "Epoch 14/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 1.3849 - acc: 0.9430\n",
      "Epoch 15/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 1.2384 - acc: 0.9445\n",
      "Epoch 16/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 1.1065 - acc: 0.9473\n",
      "Epoch 17/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.9773 - acc: 0.9516\n",
      "Epoch 18/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.8854 - acc: 0.9503\n",
      "Epoch 19/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.7642 - acc: 0.9532\n",
      "Epoch 20/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.6759 - acc: 0.9555\n",
      "Epoch 21/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.5894 - acc: 0.9548\n",
      "Epoch 22/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.4961 - acc: 0.9588\n",
      "Epoch 23/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.4307 - acc: 0.9594\n",
      "Epoch 24/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.3782 - acc: 0.9601\n",
      "Epoch 25/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.3306 - acc: 0.9618\n",
      "Epoch 26/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2999 - acc: 0.9616\n",
      "Epoch 27/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2801 - acc: 0.9608\n",
      "Epoch 28/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2568 - acc: 0.9644\n",
      "Epoch 29/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2453 - acc: 0.9631\n",
      "Epoch 30/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2346 - acc: 0.9650\n",
      "Epoch 31/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2280 - acc: 0.9657\n",
      "Epoch 32/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2235 - acc: 0.9645\n",
      "Epoch 33/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2186 - acc: 0.9651\n",
      "Epoch 34/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2175 - acc: 0.9654\n",
      "Epoch 35/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2109 - acc: 0.9646\n",
      "Epoch 36/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2116 - acc: 0.9652\n",
      "Epoch 37/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2103 - acc: 0.9657\n",
      "Epoch 38/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2099 - acc: 0.9649\n",
      "Epoch 39/50\n",
      "26880/26880 [==============================] - 2s 85us/step - loss: 0.2087 - acc: 0.9658\n",
      "Epoch 40/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2037 - acc: 0.9667\n",
      "Epoch 41/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2028 - acc: 0.9676\n",
      "Epoch 42/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2051 - acc: 0.9654\n",
      "Epoch 43/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2005 - acc: 0.9682\n",
      "Epoch 44/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2024 - acc: 0.9663\n",
      "Epoch 45/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2005 - acc: 0.9661\n",
      "Epoch 46/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2014 - acc: 0.9660\n",
      "Epoch 47/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.1993 - acc: 0.9670\n",
      "Epoch 48/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.1974 - acc: 0.9679\n",
      "Epoch 49/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.1995 - acc: 0.9672\n",
      "Epoch 50/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.1957 - acc: 0.9684\n",
      "6720/6720 [==============================] - 0s 37us/step\n",
      "[CV]  batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28, score=0.954, total= 1.9min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  7.8min remaining:    0.0s\n",
      "[CV] batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28 \n",
      "Epoch 1/50\n",
      "26880/26880 [==============================] - 3s 95us/step - loss: 11.8701 - acc: 0.4698\n",
      "Epoch 2/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 9.2540 - acc: 0.5883\n",
      "Epoch 3/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 8.0364 - acc: 0.6348\n",
      "Epoch 4/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 6.2352 - acc: 0.7269\n",
      "Epoch 5/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 5.7957 - acc: 0.7399\n",
      "Epoch 6/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 5.4844 - acc: 0.7487\n",
      "Epoch 7/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 5.2544 - acc: 0.7528\n",
      "Epoch 8/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 5.0720 - acc: 0.7556\n",
      "Epoch 9/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 4.8865 - acc: 0.7587\n",
      "Epoch 10/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 4.7424 - acc: 0.7608\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26880/26880 [==============================] - 2s 83us/step - loss: 4.5769 - acc: 0.7649\n",
      "Epoch 12/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 4.4525 - acc: 0.7669\n",
      "Epoch 13/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 4.3318 - acc: 0.7688\n",
      "Epoch 14/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 4.1980 - acc: 0.7718\n",
      "Epoch 15/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 4.0829 - acc: 0.7734\n",
      "Epoch 16/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 3.9997 - acc: 0.7742\n",
      "Epoch 17/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 3.8324 - acc: 0.7769\n",
      "Epoch 18/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 1.0768 - acc: 0.9106\n",
      "Epoch 19/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.7495 - acc: 0.9351\n",
      "Epoch 20/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.6015 - acc: 0.9449\n",
      "Epoch 21/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.4955 - acc: 0.9517\n",
      "Epoch 22/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.4164 - acc: 0.9554\n",
      "Epoch 23/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.3639 - acc: 0.9566\n",
      "Epoch 24/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.3207 - acc: 0.9592\n",
      "Epoch 25/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2932 - acc: 0.9604\n",
      "Epoch 26/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2714 - acc: 0.9609\n",
      "Epoch 27/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2558 - acc: 0.9629\n",
      "Epoch 28/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2428 - acc: 0.9631\n",
      "Epoch 29/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2345 - acc: 0.9638\n",
      "Epoch 30/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2293 - acc: 0.9633\n",
      "Epoch 31/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2221 - acc: 0.9657\n",
      "Epoch 32/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2208 - acc: 0.9650\n",
      "Epoch 33/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2164 - acc: 0.9669\n",
      "Epoch 34/50\n",
      "26880/26880 [==============================] - 2s 89us/step - loss: 0.2146 - acc: 0.9646\n",
      "Epoch 35/50\n",
      "26880/26880 [==============================] - 2s 91us/step - loss: 0.2115 - acc: 0.9659\n",
      "Epoch 36/50\n",
      "26880/26880 [==============================] - 2s 83us/step - loss: 0.2095 - acc: 0.9653\n",
      "Epoch 37/50\n",
      "26880/26880 [==============================] - 2s 84us/step - loss: 0.2078 - acc: 0.9655\n",
      "Epoch 38/50\n",
      "26880/26880 [==============================] - 2s 88us/step - loss: 0.2049 - acc: 0.9673\n",
      "Epoch 39/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2043 - acc: 0.9668\n",
      "Epoch 40/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2025 - acc: 0.9660\n",
      "Epoch 41/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2047 - acc: 0.9675\n",
      "Epoch 42/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2039 - acc: 0.9668\n",
      "Epoch 43/50\n",
      "26880/26880 [==============================] - 2s 87us/step - loss: 0.2006 - acc: 0.9674\n",
      "Epoch 44/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.1995 - acc: 0.9676\n",
      "Epoch 45/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2026 - acc: 0.9661\n",
      "Epoch 46/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2013 - acc: 0.9666\n",
      "Epoch 47/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.2005 - acc: 0.9681\n",
      "Epoch 48/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.1993 - acc: 0.9679\n",
      "Epoch 49/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.1999 - acc: 0.9668\n",
      "Epoch 50/50\n",
      "26880/26880 [==============================] - 2s 86us/step - loss: 0.1983 - acc: 0.9665\n",
      "6720/6720 [==============================] - 0s 38us/step\n",
      "[CV]  batch_size=16, l2reg=0.08829379363258615, learning_rate=0.00015013199473528794, units=28, score=0.955, total= 1.9min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  9.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  9.7min finished\n",
      "Epoch 1/50\n",
      "33600/33600 [==============================] - ETA: 0s - loss: 10.7805 - acc: 0.51 - 3s 103us/step - loss: 10.7410 - acc: 0.5188\n",
      "Epoch 2/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 6.9688 - acc: 0.7072\n",
      "Epoch 3/50\n",
      "33600/33600 [==============================] - 3s 92us/step - loss: 6.0820 - acc: 0.7337\n",
      "Epoch 4/50\n",
      "33600/33600 [==============================] - 3s 96us/step - loss: 5.2240 - acc: 0.7602\n",
      "Epoch 5/50\n",
      "33600/33600 [==============================] - 3s 97us/step - loss: 3.9729 - acc: 0.8215\n",
      "Epoch 6/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 3.6287 - acc: 0.8331\n",
      "Epoch 7/50\n",
      "33600/33600 [==============================] - 3s 95us/step - loss: 3.3683 - acc: 0.8410\n",
      "Epoch 8/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 3.1323 - acc: 0.8454\n",
      "Epoch 9/50\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 1.6129 - acc: 0.9258\n",
      "Epoch 10/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.2750 - acc: 0.9418\n",
      "Epoch 11/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 1.0992 - acc: 0.9449\n",
      "Epoch 12/50\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.9649 - acc: 0.9477\n",
      "Epoch 13/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.8237 - acc: 0.9499\n",
      "Epoch 14/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.7110 - acc: 0.9518\n",
      "Epoch 15/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.6093 - acc: 0.9528\n",
      "Epoch 16/50\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.5098 - acc: 0.9534\n",
      "Epoch 17/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.4375 - acc: 0.9552\n",
      "Epoch 18/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3797 - acc: 0.9564\n",
      "Epoch 19/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3314 - acc: 0.9592\n",
      "Epoch 20/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.3006 - acc: 0.9595\n",
      "Epoch 21/50\n",
      "33600/33600 [==============================] - 3s 89us/step - loss: 0.2754 - acc: 0.9603\n",
      "Epoch 22/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2548 - acc: 0.9603\n",
      "Epoch 23/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2481 - acc: 0.9612\n",
      "Epoch 24/50\n",
      "33600/33600 [==============================] - 3s 96us/step - loss: 0.2325 - acc: 0.9636\n",
      "Epoch 25/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2323 - acc: 0.9618\n",
      "Epoch 26/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2261 - acc: 0.9619\n",
      "Epoch 27/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2199 - acc: 0.9640\n",
      "Epoch 28/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2191 - acc: 0.9631\n",
      "Epoch 29/50\n",
      "33600/33600 [==============================] - 3s 93us/step - loss: 0.2143 - acc: 0.9654\n",
      "Epoch 30/50\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.2147 - acc: 0.9638\n",
      "Epoch 31/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2140 - acc: 0.9649\n",
      "Epoch 32/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2128 - acc: 0.9640\n",
      "Epoch 33/50\n",
      "33600/33600 [==============================] - 3s 91us/step - loss: 0.2101 - acc: 0.9643\n",
      "Epoch 34/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2112 - acc: 0.9643\n",
      "Epoch 35/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2081 - acc: 0.9653\n",
      "Epoch 36/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2066 - acc: 0.9651\n",
      "Epoch 37/50\n",
      "33600/33600 [==============================] - 3s 90us/step - loss: 0.2056 - acc: 0.9659\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2083 - acc: 0.9650\n",
      "Epoch 39/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2048 - acc: 0.9663\n",
      "Epoch 40/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2058 - acc: 0.9651\n",
      "Epoch 41/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2056 - acc: 0.9665\n",
      "Epoch 42/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2026 - acc: 0.9661\n",
      "Epoch 43/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2058 - acc: 0.9652\n",
      "Epoch 44/50\n",
      "33600/33600 [==============================] - 3s 87us/step - loss: 0.2028 - acc: 0.9672\n",
      "Epoch 45/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2025 - acc: 0.9660\n",
      "Epoch 46/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2027 - acc: 0.9657\n",
      "Epoch 47/50\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2029 - acc: 0.9654\n",
      "Epoch 48/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2046 - acc: 0.9659\n",
      "Epoch 49/50\n",
      "33600/33600 [==============================] - 3s 86us/step - loss: 0.2009 - acc: 0.9668\n",
      "Epoch 50/50\n",
      "33600/33600 [==============================] - 3s 85us/step - loss: 0.2029 - acc: 0.9671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x000000001091F198>,\n",
       "                   iid='warn', n_iter=1, n_jobs=1,\n",
       "                   param_distributions={'batch_size': [16, 32, 64, 128],\n",
       "                                        'l2reg': <__main__.LogUniform object at 0x000000001091F278>,\n",
       "                                        'learning_rate': <__main__.LogUniform object at 0x000000001091F1D0>,\n",
       "                                        'units': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000000001091F2B0>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(learning_rate, units, l2reg=0, hidden_layers=1):\n",
    "    # cleanup\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units, activation='relu', input_dim=X_train.shape[1], kernel_regularizer=l2(l2reg)))\n",
    "    for i in range(hidden_layers - 1):\n",
    "        model.add(Dense(units=units, activation='relu', kernel_regularizer=l2(l2reg)))\n",
    "    \n",
    "    model.add(Dense(units=10, activation=\"softmax\", kernel_regularizer=l2(l2reg)))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=50)\n",
    "param_dist = {\"learning_rate\": LogUniform(0.0001, 1),\n",
    "              \"l2reg\": LogUniform(0.001, 1),\n",
    "              \"units\": randint(20, 200),\n",
    "              \"batch_size\": [16, 32, 64, 128],\n",
    "              #\"hidden_layers\": [1]\n",
    "             }\n",
    "clf = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_jobs=1, cv=5, n_iter=20, verbose=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# l2reg = 0.01\n",
    "# model = Sequential()\n",
    "# model.add(Dense(units=100, activation='relu', input_dim=X_train.shape[1], kernel_regularizer=l2(l2reg)))\n",
    "# model.add(Dense(units=10, activation=\"softmax\", kernel_regularizer=l2(l2reg)))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#           optimizer=Adam(0.003),\n",
    "#           metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16, 'l2reg': 0.08829379363258615, 'learning_rate': 0.00015013199473528794, 'units': 28}\n",
      "33600/33600 [==============================] - 1s 34us/step\n",
      "8400/8400 [==============================] - 0s 34us/step\n",
      "0.9739880952380953 0.9622619047619048\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(train_score, test_score)\n",
    "\n",
    "# train_loss_and_metrics = model.evaluate(X_train, y_train)\n",
    "# test_loss_and_metrics = model.evaluate(X_test, y_test)\n",
    "# print(train_loss_and_metrics, test_loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "predictions = clf.predict(df_test)\n",
    "# predictions = np.squeeze(onehot.inverse_transform(predictions))\n",
    "df_submit = pd.DataFrame({'ImageId': list(range(1, 28001)), 'Label': predictions})\n",
    "df_submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
